{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import matplotlib\n",
    "plt.style.use(\"seaborn-v0_8-deep\")\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,  # Use LaTeX for text rendering\n",
    "    \"font.family\": \"serif\",  # Use serif font\n",
    "    \"font.serif\": [\"Computer Modern Roman\"],  # Use default LaTeX font\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lstm = pd.read_csv('/tikhome/tfellner/Projects/VQA_timeseries_benchmark/Results/lstm_hyper_opt.csv')\n",
    "df_rnn = pd.read_csv('/tikhome/tfellner/Projects/VQA_timeseries_benchmark/Results/rnn_hyper_opt.csv')\n",
    "df_vqc = pd.read_csv('/tikhome/tfellner/Projects/VQA_timeseries_benchmark/Results/vqc_hyper_opt.csv')\n",
    "df_mlp = pd.read_csv('/tikhome/tfellner/Projects/VQA_timeseries_benchmark/Results/mlp_hyper_opt.csv')\n",
    "df_qrnn = pd.read_csv('/tikhome/tfellner/Projects/VQA_timeseries_benchmark/Results/qrnn_paper_hyper_opt.csv')\n",
    "df_qlstm_paper = pd.read_csv('/tikhome/tfellner/Projects/VQA_timeseries_benchmark/Results/qlstm_paper_hyper_opt.csv')\n",
    "df_qlstm_le = pd.read_csv('/tikhome/tfellner/Projects/VQA_timeseries_benchmark/Results/qlstm_linear_enhanced_paper_hyper_opt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vqc_reuploading_expencnorm = df_vqc[df_vqc['Ansatz'].str.startswith('ruexp_')]\n",
    "df_vqc_plain = df_vqc[df_vqc['Ansatz'].str.startswith('paper_rivera-ruiz_no_inputlayer_')]\n",
    "df_vqc_dressed = df_vqc[df_vqc['Ansatz'].str.startswith('paper_rivera-ruiz_with_inputlayer_')]\n",
    "df_qrnn_no_reset = df_qrnn[df_qrnn['Ansatz']== 'paper_no_reset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best(df):\n",
    "    df_intern = df.copy()\n",
    "    best_rows = df_intern.loc[df_intern.groupby(['Prediction Step', 'Data', 'Sequence Length'], observed=True)['MSE Validation Median'].idxmin()]\n",
    "    return best_rows\n",
    "\n",
    "best_lstm = find_best(df_lstm)\n",
    "best_vqc_reuploading_expencnorm = find_best(df_vqc_reuploading_expencnorm)\n",
    "best_vqc_plain = find_best(df_vqc_plain)\n",
    "best_vqc_dressed = find_best(df_vqc_dressed)\n",
    "best_mlp = find_best(df_mlp)\n",
    "best_qrnn_no_reset = find_best(df_qrnn_no_reset)\n",
    "best_qlstm_paper = find_best(df_qlstm_paper)\n",
    "best_rnn = find_best(df_rnn)\n",
    "best_qlstm_le = find_best(df_qlstm_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_lengths = [4, 8, 16]\n",
    "prediction_steps_mackey = [1, 70, 140]\n",
    "prediction_steps_henon = [1, 2, 4]\n",
    "prediction_steps_lorenz = [1, 13, 25]\n",
    "models = {'p-VQC': (best_vqc_plain, '#6A3D9A'),'d-VQC': (best_vqc_dressed, '#CCB974'), 'ru-VQC': (best_vqc_reuploading_expencnorm, '#8172B2'), 'QRNN': (best_qrnn_no_reset, '#C44E52'), 'QLSTM': (best_qlstm_paper, '#64B5CD'), 'le-QLSTM': (best_qlstm_le, '#4C72B0'), 'MLP': (best_mlp, '#E377C2'),'RNN': (best_rnn, '#55A868'), 'LSTM': (best_lstm, '#8C564B')}\n",
    "data = {'mackey_1000': (prediction_steps_mackey, 'Mackey-Glass'), 'henon_1000': (prediction_steps_henon, 'HÃ©non'), 'lorenz_1000': (prediction_steps_lorenz, 'Lorenz')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'p-QNN': (best_vqc_plain, '#6A3D9A'),\n",
    "    'd-QNN': (best_vqc_dressed, '#CCB974'),\n",
    "    'ru-QNN': (best_vqc_reuploading_expencnorm, '#8172B2'),\n",
    "    'QRNN': (best_qrnn_no_reset, '#C44E52'),\n",
    "    'QLSTM': (best_qlstm_paper, '#64B5CD'),\n",
    "    'le-QLSTM': (best_qlstm_le, '#4C72B0'),\n",
    "    'MLP': (best_mlp, '#E377C2'),\n",
    "    'RNN': (best_rnn, '#55A868'),\n",
    "    'LSTM': (best_lstm, '#8C564B')\n",
    "}\n",
    "\n",
    "#omitting p-VQC:\n",
    "models = {\n",
    "    'd-QNN': (best_vqc_dressed, '#CCB974'),\n",
    "    'ru-QNN': (best_vqc_reuploading_expencnorm, '#8172B2'),\n",
    "    'QRNN': (best_qrnn_no_reset, '#C44E52'),\n",
    "    'QLSTM': (best_qlstm_paper, '#64B5CD'),\n",
    "    'le-QLSTM': (best_qlstm_le, '#4C72B0'),\n",
    "    'MLP': (best_mlp, '#E377C2'),\n",
    "    'RNN': (best_rnn, '#55A868'),\n",
    "    'LSTM': (best_lstm, '#8C564B')\n",
    "}\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "# Add model name as a new column\n",
    "for model_name, (df, color) in models.items():\n",
    "    df_copy = df.copy()  # Avoid modifying the original DataFrame\n",
    "    df_copy['Model Name'] = model_name\n",
    "    dataframes.append(df_copy)\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "results_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "results_df['Rank'] = results_df.groupby(['Sequence Length', 'Data', 'Prediction Step'])['MSE Testing Median'].rank(method='min')\n",
    "\n",
    "# Sort the DataFrame for better visualization\n",
    "results_df = results_df.sort_values(by=['Sequence Length', 'Data', 'Prediction Step', 'Rank'])\n",
    "\n",
    "# Count the number of times each model has rankings 1, 2, 3, 4, and 5\n",
    "rank_counts = results_df[results_df['Rank'] <= 9].groupby(['Model Name', 'Rank']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean ranking for each model\n",
    "mean_rankings = results_df.groupby('Model Name')['Rank'].mean()\n",
    "\n",
    "# Sort the models by their mean rank\n",
    "sorted_models = mean_rankings.sort_values(ascending=False).index\n",
    "\n",
    "# Reorder rank_counts according to the sorted models\n",
    "rank_counts_sorted = rank_counts.loc[sorted_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a color map from red (worst place) to blue (best place)\n",
    "cmap = plt.colormaps.get_cmap('RdYlGn_r')\n",
    "# Generate 9 evenly spaced colors from the colormap\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 8)] # change to np.linspace(0, 1, 9) for all models\n",
    "\n",
    "# Sort the models by the number of rank 1 in descending order\n",
    "# rank_counts = rank_counts.sort_values(by=[1.0, 2.0, 3.0, 4.0], ascending=[True, True, True, True])\n",
    "plt.figure(figsize=(4, 3))\n",
    "ax = rank_counts_sorted.plot(kind='barh', stacked=True, color=[colors[i] for i in range(8)], fontsize=14)# change to range(9) for all models\n",
    "plt.xlabel('Number of Rankings', fontsize=14)\n",
    "plt.ylabel('Model', fontsize=14)\n",
    "\n",
    "# Place the legend outside and set the labels as integers\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "labels = [str(int(float(label))) for label in labels]  # Convert float labels to integers\n",
    "ax.legend(handles, labels, title='Ranking', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=14)\n",
    "#ax.set_xticks(range(int(rank_counts.sum(axis=1).max() + 1)))  # Adjust the range based on your data\n",
    "max_value = int(rank_counts.sum(axis=1).max()+1)\n",
    "xticks = np.arange(0, max_value +1, 3)  # Create ticks every 2 units\n",
    "ax.set_xticks(xticks)\n",
    "\n",
    "# Enable grid only for x-axis\n",
    "plt.grid(axis='x', which='major')\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'/tikhome/tfellner/Projects/VQA_timeseries_benchmark/Plots/Rankings/all_models_mse_ranking_all_ruopt.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a color map from red (worst place) to blue (best place)\n",
    "cmap = plt.colormaps.get_cmap('coolwarm')\n",
    "# Generate 9 evenly spaced colors from the colormap\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 8)] # change to np.linspace(0, 1, 9) for all models\n",
    "\n",
    "# Sort the models by the number of rank 1 in descending order\n",
    "# rank_counts = rank_counts.sort_values(by=[1.0, 2.0, 3.0, 4.0], ascending=[True, True, True, True])\n",
    "plt.figure(figsize=(4, 3))\n",
    "ax = rank_counts_sorted.plot(kind='barh', stacked=True, color=[colors[i] for i in range(8)], fontsize=14) # change to range(9) for all models\n",
    "plt.xlabel('Number of Rankings', fontsize=14)\n",
    "plt.ylabel('Model', fontsize=14)\n",
    "\n",
    "# Place the legend outside and set the labels as integers\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "labels = [str(int(float(label))) for label in labels]  # Convert float labels to integers\n",
    "ax.legend(handles, labels, title='Ranking', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=14)\n",
    "#ax.set_xticks(range(int(rank_counts.sum(axis=1).max() + 1)))  # Adjust the range based on your data\n",
    "max_value = int(rank_counts.sum(axis=1).max()+1)\n",
    "xticks = np.arange(0, max_value +1, 3)  # Create ticks every 2 units\n",
    "ax.set_xticks(xticks)\n",
    "\n",
    "# Enable grid only for x-axis\n",
    "plt.grid(axis='x', which='major')\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'/tikhome/tfellner/Projects/VQA_timeseries_benchmark/Plots/Rankings/all_models_mse_rankin_red_blue_all_ruopt.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
